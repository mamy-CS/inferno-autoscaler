# Controller deployment settings
# Set controller.enabled=false to deploy only VA/HPA/ServiceMonitor resources
# without deploying another controller instance (use existing cluster-wide controller)
controller:
  enabled: true

wva:
  enabled: true

  image:
    repository: ghcr.io/llm-d/llm-d-workload-variant-autoscaler
    tag: v0.5.0
  imagePullPolicy: Always

  metrics:
    enabled: true
    port: 8443
    secure: true
  
  # If true, the controller will only watch the namespace it is deployed in.
  # If false, the controller will watch all namespaces (cluster-scoped).
  namespaceScoped: true

  reconcileInterval: 60s

  # ConfigMap settings
  configMap:
    # If true, makes the ConfigMap immutable (cannot be updated after creation).
    # This provides security benefits by preventing accidental or malicious changes
    # to configuration, but disables runtime config updates (dynamic config changes
    # will require recreating the ConfigMap and restarting the controller).
    # Default: false (allows runtime updates for dynamic configuration)
    immutable: false
    
  prometheus:
    monitoringNamespace: openshift-user-workload-monitoring
    serviceAccountName: "kube-prometheus-stack-prometheus"
    baseURL: "https://thanos-querier.openshift-monitoring.svc.cluster.local:9091"
    # Development security configuration (relaxed for easier development)
    tls:
      insecureSkipVerify: true   # Development: true, Production: false
      caCertPath: "/etc/ssl/certs/prometheus-ca.crt"
    # caCert: |  # Uncomment and provide your CA certificate
    #   -----BEGIN CERTIFICATE-----
    #   YOUR_CA_CERTIFICATE_HERE
    #   -----END CERTIFICATE-----

  limitedMode: false  # Enable limited mode (default: false)
  # Node selector for sharding WVA instances
  # Example: "wva.llmd.ai/shard=instance-a"
  nodeSelector: ""
  scaleToZero: false  # Enable scaling variants to zero replicas (default: false)
  # Controller instance identifier for multi-controller isolation
  # When set, adds controller_instance label to all emitted metrics
  # Used with HPA selector to filter metrics from specific controller instances
  # Useful for parallel e2e tests where multiple WVA controllers run simultaneously
  controllerInstance: ""

  # Saturation-based scaling configuration
  # These thresholds determine when replicas are saturated and when to scale up
  capacityScaling:
    # Global defaults applied to all variants unless overridden
    default:
      kvCacheThreshold: 0.80      # Replica saturated if KV cache utilization >= threshold (0.0-1.0)
      queueLengthThreshold: 5     # Replica saturated if queue length >= threshold
      kvSpareTrigger: 0.1         # Scale-up if avg spare KV capacity < trigger (0.0-1.0)
      queueSpareTrigger: 3        # Scale-up if avg spare queue capacity < trigger
    
    # Per-model/namespace overrides (optional)
    # Example:
    # overrides:
    #   llm-d:
    #     modelID: "Qwen/Qwen3-0.6B"
    #     namespace: "llm-d-autoscaler"
    #     kvCacheThreshold: 0.70
    #     kvSpareTrigger: 0.35
    overrides: {}

llmd:
  namespace: llm-d-autoscaler
  modelName: ms-workload-autoscaler-llm-d-modelservice
  modelID: "Qwen/Qwen3-0.6B"

va:
  enabled: true
  accelerator: H100
  # Cost per replica in arbitrary units (higher = more expensive to scale)
  # Used by saturation analysis to weight scaling decisions across variants
  # Example: H100=10.0, A100=8.0, L40S=5.0 (relative GPU costs)
  variantCost: "10.0"
  sloTpot: 10
  sloTtft: 1000

hpa:
  enabled: true
  # minReplicas: 0 enables scale-to-zero (requires HPAScaleToZero feature gate in k8s)
  # minReplicas: 1 is the safe default that prevents scale-to-zero
  # Set to 0 when wva.scaleToZero is enabled
  minReplicas: 1
  maxReplicas: 10
  targetAverageValue: "1"
  # HPA scaling behavior configuration
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 240
      selectPolicy: Max
      policies:
        - type: Pods
          value: 10
          periodSeconds: 150
    scaleDown:
      stabilizationWindowSeconds: 240
      selectPolicy: Max
      policies:
        - type: Pods
          value: 10
          periodSeconds: 150

vllmService:
  enabled: true
  port: 8200
  targetPort: 8200
  nodePort: 30000
  interval: 15s
  scheme: http  # vLLM emulator runs on HTTP
