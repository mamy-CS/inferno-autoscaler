name: CI - OpenShift E2E Tests

on:
  pull_request:
    branches:
      - main
      - dev
  workflow_dispatch:
    inputs:
      model_id:
        description: 'Model ID'
        required: false
        default: 'unsloth/Meta-Llama-3.1-8B'
      accelerator_type:
        description: 'Accelerator type (H100, A100, L40S)'
        required: false
        default: 'H100'
      request_rate:
        description: 'Request rate (req/s)'
        required: false
        default: '20'
      num_prompts:
        description: 'Number of prompts'
        required: false
        default: '3000'
      skip_cleanup:
        description: 'Skip cleanup after tests'
        required: false
        default: 'false'
      max_num_seqs:
        description: 'vLLM max batch size (lower = easier to saturate)'
        required: false
        default: '1'
      hpa_stabilization_seconds:
        description: 'HPA stabilization window in seconds'
        required: false
        default: '120'

jobs:
  e2e-openshift:
    runs-on: [self-hosted, openshift]
    env:
      MODEL_ID: ${{ github.event.inputs.model_id || 'unsloth/Meta-Llama-3.1-8B' }}
      ACCELERATOR_TYPE: ${{ github.event.inputs.accelerator_type || 'H100' }}
      REQUEST_RATE: ${{ github.event.inputs.request_rate || '20' }}
      NUM_PROMPTS: ${{ github.event.inputs.num_prompts || '3000' }}
      MAX_NUM_SEQS: ${{ github.event.inputs.max_num_seqs || '1' }}
      HPA_STABILIZATION_SECONDS: ${{ github.event.inputs.hpa_stabilization_seconds || '120' }}
    steps:
      - name: Checkout source
        uses: actions/checkout@v4

      - name: Extract Go version from go.mod
        run: sed -En 's/^go (.*)$/GO_VERSION=\1/p' go.mod >> $GITHUB_ENV

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: "${{ env.GO_VERSION }}"
          cache-dependency-path: ./go.sum

      - name: Install tools (kubectl, oc, helm, make)
        run: |
          sudo apt-get update && sudo apt-get install -y make
          # Install kubectl
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
          # Install oc (OpenShift CLI)
          curl -LO "https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable/openshift-client-linux.tar.gz"
          tar -xzf openshift-client-linux.tar.gz
          sudo mv oc /usr/local/bin/
          rm -f openshift-client-linux.tar.gz kubectl README.md
          # Install helm
          curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

      - name: Verify cluster access
        run: |
          echo "Verifying cluster access..."
          kubectl cluster-info
          kubectl get nodes

      - name: Deploy WVA and llm-d infrastructure
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          ENVIRONMENT: openshift
          INSTALL_GATEWAY_CTRLPLANE: "false"
          E2E_TESTS_ENABLED: "true"
          NAMESPACE_SCOPED: "false"
        run: |
          echo "Deploying WVA and llm-d infrastructure..."
          echo "  MODEL_ID: $MODEL_ID"
          echo "  ACCELERATOR_TYPE: $ACCELERATOR_TYPE"
          ./deploy/install.sh --model "$MODEL_ID" --accelerator "$ACCELERATOR_TYPE" --environment openshift

      - name: Wait for infrastructure to be ready
        run: |
          echo "Waiting for WVA controller to be ready..."
          kubectl wait --for=condition=available --timeout=300s deployment -l app.kubernetes.io/name=workload-variant-autoscaler -n workload-variant-autoscaler-system || true
          kubectl get pods -n workload-variant-autoscaler-system
          echo "Waiting for llm-d deployment to be ready..."
          kubectl get pods -n llm-d-inference-scheduler

      - name: Patch vLLM deployment for e2e testing
        run: |
          echo "Patching vLLM decode deployment to limit batch size for scaling test..."
          echo "  MAX_NUM_SEQS: $MAX_NUM_SEQS"
          DEPLOYMENT_NAME="ms-inference-scheduling-llm-d-modelservice-decode"
          # Determine the index of the target container by name instead of assuming index 0.
          # Here we assume the main container has the same name as the deployment.
          TARGET_CONTAINER_NAME="$DEPLOYMENT_NAME"
          CONTAINER_INDEX="$(
            kubectl get deployment "$DEPLOYMENT_NAME" -n llm-d-inference-scheduler \
              -o jsonpath='{range .spec.template.spec.containers[*]}{.name}{"\n"}{end}' \
            | nl -v0 -nln -w1 \
            | awk -v name="$TARGET_CONTAINER_NAME" '$2 == name {print $1; exit}'
          )"
          if [ -z "$CONTAINER_INDEX" ]; then
            echo "Error: container '$TARGET_CONTAINER_NAME' not found in deployment '$DEPLOYMENT_NAME'" >&2
            exit 1
          fi
          # Add --max-num-seqs to force scaling under load on the correct container
          kubectl patch deployment "$DEPLOYMENT_NAME" -n llm-d-inference-scheduler --type=json -p="[
            {\"op\": \"add\", \"path\": \"/spec/template/spec/containers/$CONTAINER_INDEX/args/-\", \"value\": \"--max-num-seqs=$MAX_NUM_SEQS\"}
          ]"
          echo "Waiting for patched deployment to roll out..."
          kubectl rollout status deployment/"$DEPLOYMENT_NAME" -n llm-d-inference-scheduler --timeout=300s

      - name: Patch HPA for faster e2e testing
        run: |
          echo "Patching HPA stabilization window for e2e testing..."
          echo "  HPA_STABILIZATION_SECONDS: $HPA_STABILIZATION_SECONDS"
          kubectl patch hpa vllm-deployment-hpa -n llm-d-inference-scheduler --type=json -p='[
            {"op": "replace", "path": "/spec/behavior/scaleUp/stabilizationWindowSeconds", "value": '"$HPA_STABILIZATION_SECONDS"'},
            {"op": "replace", "path": "/spec/behavior/scaleDown/stabilizationWindowSeconds", "value": '"$HPA_STABILIZATION_SECONDS"'}
          ]'
          kubectl get hpa -n llm-d-inference-scheduler

      - name: Install Go dependencies
        run: go mod download

      - name: Run OpenShift E2E tests
        env:
          CONTROLLER_NAMESPACE: workload-variant-autoscaler-system
          MONITORING_NAMESPACE: openshift-user-workload-monitoring
          LLMD_NAMESPACE: llm-d-inference-scheduler
          GATEWAY_NAME: infra-inference-scheduling-inference-gateway-istio
          DEPLOYMENT: ms-inference-scheduling-llm-d-modelservice-decode
        run: |
          echo "Running OpenShift E2E tests with configuration:"
          echo "  CONTROLLER_NAMESPACE: $CONTROLLER_NAMESPACE"
          echo "  LLMD_NAMESPACE: $LLMD_NAMESPACE"
          echo "  DEPLOYMENT: $DEPLOYMENT"
          echo "  GATEWAY_NAME: $GATEWAY_NAME"
          echo "  MODEL_ID: $MODEL_ID"
          echo "  REQUEST_RATE: $REQUEST_RATE"
          echo "  NUM_PROMPTS: $NUM_PROMPTS"
          make test-e2e-openshift

      - name: Cleanup infrastructure
        if: always() && github.event.inputs.skip_cleanup != 'true'
        env:
          ENVIRONMENT: openshift
        run: |
          echo "Cleaning up infrastructure..."
          ./deploy/install.sh --undeploy --environment openshift || true
